game: othello

GameType.chance_mode = ChanceMode.DETERMINISTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Othello"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = []
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "othello"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 65
PolicyTensorShape() = [65]
MaxChanceOutcomes() = 0
GetParameters() = {}
NumPlayers() = 2
MinUtility() = -1.0
MaxUtility() = 1.0
UtilitySum() = 0.0
ObservationTensorShape() = [3, 8, 8]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 192
MaxGameLength() = 64
ToString() = "othello()"

# State 0
# Black (x) to play:
#   a b c d e f g h
# 1 - - - - - - - - 1
# 2 - - - - - - - - 2
# 3 - - - - - - - - 3
# 4 - - - o x - - - 4
# 5 - - - x o - - - 5
# 6 - - - - - - - - 6
# 7 - - - - - - - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = ""
InformationStateString(1) = ""
ObservationString(0) = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - x o - - - 5\n6 - - - - - - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - x o - - - 5\n6 - - - - - - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - x o - - - 5\n6 - - - - - - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [19, 26, 37, 44]
StringLegalActions() = ["d3", "c4", "f5", "e6"]

# Apply action "e6"
action: 44

# State 1
# White (o) to play:
#   a b c d e f g h
# 1 - - - - - - - - 1
# 2 - - - - - - - - 2
# 3 - - - - - - - - 3
# 4 - - - o x - - - 4
# 5 - - - x x - - - 5
# 6 - - - - x - - - 6
# 7 - - - - - - - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44]
HistoryString() = "44"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "44"
InformationStateString(1) = "44"
ObservationString(0) = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - x x - - - 5\n6 - - - - x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - x x - - - 5\n6 - - - - x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - x x - - - 5\n6 - - - - x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◉◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◉◉◯◯◯
◉◉◉◉◯◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [29, 43, 45]
StringLegalActions() = ["f4", "d6", "f6"]

# Apply action "d6"
action: 43

# State 2
# Black (x) to play:
#   a b c d e f g h
# 1 - - - - - - - - 1
# 2 - - - - - - - - 2
# 3 - - - - - - - - 3
# 4 - - - o x - - - 4
# 5 - - - o x - - - 5
# 6 - - - o x - - - 6
# 7 - - - - - - - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43]
HistoryString() = "44, 43"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 43"
InformationStateString(1) = "44, 43"
ObservationString(0) = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - o x - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - o x - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - - o x - - - 4\n5 - - - o x - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [18, 26, 34, 42, 50]
StringLegalActions() = ["c3", "c4", "c5", "c6", "c7"]

# Apply action "c4"
action: 26

# State 3
# White (o) to play:
#   a b c d e f g h
# 1 - - - - - - - - 1
# 2 - - - - - - - - 2
# 3 - - - - - - - - 3
# 4 - - x x x - - - 4
# 5 - - - x x - - - 5
# 6 - - - o x - - - 6
# 7 - - - - - - - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26]
HistoryString() = "44, 43, 26"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "44, 43, 26"
InformationStateString(1) = "44, 43, 26"
ObservationString(0) = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x - - - 4\n5 - - - x x - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x - - - 4\n5 - - - x x - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x - - - 4\n5 - - - x x - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◯◯◯◉◉◉  ◯◯◉◉◉◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◉◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◯◯◯◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◉◉◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◉◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [19, 29, 45]
StringLegalActions() = ["d3", "f4", "f6"]

# Apply action "f4"
action: 29

# State 4
# Black (x) to play:
#   a b c d e f g h
# 1 - - - - - - - - 1
# 2 - - - - - - - - 2
# 3 - - - - - - - - 3
# 4 - - x x x o - - 4
# 5 - - - x o - - - 5
# 6 - - - o x - - - 6
# 7 - - - - - - - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26, 29]
HistoryString() = "44, 43, 26, 29"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 43, 26, 29"
InformationStateString(1) = "44, 43, 26, 29"
ObservationString(0) = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x o - - 4\n5 - - - x o - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x o - - 4\n5 - - - x o - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "Black (x) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x o - - 4\n5 - - - x o - - - 5\n6 - - - o x - - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◯◯◯◯◉◉  ◯◯◉◉◉◯◯◯  ◯◯◯◯◯◉◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◯◯◯◯◉◉  ◯◯◯◯◯◉◯◯  ◯◯◉◉◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◉◯◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [30, 37, 42, 45, 51]
StringLegalActions() = ["g4", "f5", "c6", "f6", "d7"]

# Apply action "f6"
action: 45

# State 5
# White (o) to play:
#   a b c d e f g h
# 1 - - - - - - - - 1
# 2 - - - - - - - - 2
# 3 - - - - - - - - 3
# 4 - - x x x o - - 4
# 5 - - - x x - - - 5
# 6 - - - o x x - - 6
# 7 - - - - - - - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26, 29, 45]
HistoryString() = "44, 43, 26, 29, 45"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "44, 43, 26, 29, 45"
InformationStateString(1) = "44, 43, 26, 29, 45"
ObservationString(0) = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x o - - 4\n5 - - - x x - - - 5\n6 - - - o x x - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x o - - 4\n5 - - - x x - - - 5\n6 - - - o x x - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "White (o) to play:\n  a b c d e f g h  \n1 - - - - - - - - 1\n2 - - - - - - - - 2\n3 - - - - - - - - 3\n4 - - x x x o - - 4\n5 - - - x x - - - 5\n6 - - - o x x - - 6\n7 - - - - - - - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◯◯◯◯◉◉  ◯◯◉◉◉◯◯◯  ◯◯◯◯◯◉◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◉◉◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◯◯◯◉◉  ◯◯◯◯◉◉◯◯  ◯◯◯◉◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◯◯◯◯◉◉  ◯◯◯◯◯◉◯◯  ◯◯◉◉◉◯◯◯
◉◉◉◯◯◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◉◉◯◯◯
◉◉◉◯◯◯◉◉  ◯◯◯◉◯◯◯◯  ◯◯◯◯◉◉◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [19, 25, 46]
StringLegalActions() = ["d3", "b4", "g6"]

# Apply action "b4"
action: 25

# State 6
# Apply action "b3"
action: 17

# State 7
# Apply action "e7"
action: 52

# State 8
# Apply action "f3"
action: 21

# State 9
# Apply action "f2"
action: 13

# State 10
# Apply action "f5"
action: 37

# State 11
# Apply action "c6"
action: 42

# State 12
# Apply action "b6"
action: 41

# State 13
# Apply action "f7"
action: 53

# State 14
# Apply action "c5"
action: 34

# State 15
# Apply action "a2"
action: 8

# State 16
# Apply action "a3"
action: 16

# State 17
# Apply action "c7"
action: 50

# State 18
# Apply action "a1"
action: 0

# State 19
# Apply action "a7"
action: 48

# State 20
# Black (x) to play:
#   a b c d e f g h
# 1 x - - - - - - - 1
# 2 x - - - - o - - 2
# 3 x o - - - o - - 3
# 4 - x o o o o - - 4
# 5 - - o o o o - - 5
# 6 - o o o o o - - 6
# 7 o - o - o o - - 7
# 8 - - - - - - - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48]
HistoryString() = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48"
InformationStateString(1) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48"
ObservationString(0) = "Black (x) to play:\n  a b c d e f g h  \n1 x - - - - - - - 1\n2 x - - - - o - - 2\n3 x o - - - o - - 3\n4 - x o o o o - - 4\n5 - - o o o o - - 5\n6 - o o o o o - - 6\n7 o - o - o o - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
ObservationString(1) = "Black (x) to play:\n  a b c d e f g h  \n1 x - - - - - - - 1\n2 x - - - - o - - 2\n3 x o - - - o - - 3\n4 - x o o o o - - 4\n5 - - o o o o - - 5\n6 - o o o o o - - 6\n7 o - o - o o - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PublicObservationString() = "Black (x) to play:\n  a b c d e f g h  \n1 x - - - - - - - 1\n2 x - - - - o - - 2\n3 x o - - - o - - 3\n4 - x o o o o - - 4\n5 - - o o o o - - 5\n6 - o o o o o - - 6\n7 o - o - o o - - 7\n8 - - - - - - - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◯◉◉◉◉◉◉◉  ◉◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◯◉◉◉◉◯◉◉  ◉◯◯◯◯◯◯◯  ◯◯◯◯◯◉◯◯
◯◯◉◉◉◯◉◉  ◉◯◯◯◯◯◯◯  ◯◉◯◯◯◉◯◯
◉◯◯◯◯◯◉◉  ◯◉◯◯◯◯◯◯  ◯◯◉◉◉◉◯◯
◉◉◯◯◯◯◉◉  ◯◯◯◯◯◯◯◯  ◯◯◉◉◉◉◯◯
◉◯◯◯◯◯◉◉  ◯◯◯◯◯◯◯◯  ◯◉◉◉◉◉◯◯
◯◉◯◉◯◯◉◉  ◯◯◯◯◯◯◯◯  ◉◯◉◯◉◉◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◯◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◉◯◯◯◯◯◯◯
◯◉◉◉◉◯◉◉  ◯◯◯◯◯◉◯◯  ◉◯◯◯◯◯◯◯
◯◯◉◉◉◯◉◉  ◯◉◯◯◯◉◯◯  ◉◯◯◯◯◯◯◯
◉◯◯◯◯◯◉◉  ◯◯◉◉◉◉◯◯  ◯◉◯◯◯◯◯◯
◉◉◯◯◯◯◉◉  ◯◯◉◉◉◉◯◯  ◯◯◯◯◯◯◯◯
◉◯◯◯◯◯◉◉  ◯◉◉◉◉◉◯◯  ◯◯◯◯◯◯◯◯
◯◉◯◉◯◯◉◉  ◉◯◉◯◉◉◯◯  ◯◯◯◯◯◯◯◯
◉◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [9, 18, 30, 61, 62]
StringLegalActions() = ["b2", "c3", "g4", "f8", "g8"]

# Apply action "f8"
action: 61

# State 21
# White (o) to play:
#   a b c d e f g h
# 1 x - - - - - - - 1
# 2 x - - - - o - - 2
# 3 x o - - - o - - 3
# 4 - x o o o o - - 4
# 5 - - x o o o - - 5
# 6 - o o x o o - - 6
# 7 o - o - x o - - 7
# 8 - - - - - x - - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61]
HistoryString() = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61"
InformationStateString(1) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61"
ObservationString(0) = "White (o) to play:\n  a b c d e f g h  \n1 x - - - - - - - 1\n2 x - - - - o - - 2\n3 x o - - - o - - 3\n4 - x o o o o - - 4\n5 - - x o o o - - 5\n6 - o o x o o - - 6\n7 o - o - x o - - 7\n8 - - - - - x - - 8\n  a b c d e f g h  "
ObservationString(1) = "White (o) to play:\n  a b c d e f g h  \n1 x - - - - - - - 1\n2 x - - - - o - - 2\n3 x o - - - o - - 3\n4 - x o o o o - - 4\n5 - - x o o o - - 5\n6 - o o x o o - - 6\n7 o - o - x o - - 7\n8 - - - - - x - - 8\n  a b c d e f g h  "
PublicObservationString() = "White (o) to play:\n  a b c d e f g h  \n1 x - - - - - - - 1\n2 x - - - - o - - 2\n3 x o - - - o - - 3\n4 - x o o o o - - 4\n5 - - x o o o - - 5\n6 - o o x o o - - 6\n7 o - o - x o - - 7\n8 - - - - - x - - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◯◉◉◉◉◉◉◉  ◉◯◯◯◯◯◯◯  ◯◯◯◯◯◯◯◯
◯◉◉◉◉◯◉◉  ◉◯◯◯◯◯◯◯  ◯◯◯◯◯◉◯◯
◯◯◉◉◉◯◉◉  ◉◯◯◯◯◯◯◯  ◯◉◯◯◯◉◯◯
◉◯◯◯◯◯◉◉  ◯◉◯◯◯◯◯◯  ◯◯◉◉◉◉◯◯
◉◉◯◯◯◯◉◉  ◯◯◉◯◯◯◯◯  ◯◯◯◉◉◉◯◯
◉◯◯◯◯◯◉◉  ◯◯◯◉◯◯◯◯  ◯◉◉◯◉◉◯◯
◯◉◯◉◯◯◉◉  ◯◯◯◯◉◯◯◯  ◉◯◉◯◯◉◯◯
◉◉◉◉◉◯◉◉  ◯◯◯◯◯◉◯◯  ◯◯◯◯◯◯◯◯
ObservationTensor(1):
◯◉◉◉◉◉◉◉  ◯◯◯◯◯◯◯◯  ◉◯◯◯◯◯◯◯
◯◉◉◉◉◯◉◉  ◯◯◯◯◯◉◯◯  ◉◯◯◯◯◯◯◯
◯◯◉◉◉◯◉◉  ◯◉◯◯◯◉◯◯  ◉◯◯◯◯◯◯◯
◉◯◯◯◯◯◉◉  ◯◯◉◉◉◉◯◯  ◯◉◯◯◯◯◯◯
◉◉◯◯◯◯◉◉  ◯◯◯◉◉◉◯◯  ◯◯◉◯◯◯◯◯
◉◯◯◯◯◯◉◉  ◯◉◉◯◉◉◯◯  ◯◯◯◉◯◯◯◯
◯◉◯◉◯◯◉◉  ◉◯◉◯◯◉◯◯  ◯◯◯◯◉◯◯◯
◉◉◉◉◉◯◉◉  ◯◯◯◯◯◯◯◯  ◯◯◯◯◯◉◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [24, 33, 51, 59, 60]
StringLegalActions() = ["a4", "b5", "d7", "d8", "e8"]

# Apply action "e8"
action: 60

# State 22
# Apply action "d8"
action: 59

# State 23
# Apply action "b5"
action: 33

# State 24
# Apply action "c3"
action: 18

# State 25
# Apply action "d7"
action: 51

# State 26
# Apply action "c8"
action: 58

# State 27
# Apply action "b2"
action: 9

# State 28
# Apply action "c1"
action: 2

# State 29
# Apply action "b7"
action: 49

# State 30
# Apply action "g8"
action: 62

# State 31
# Apply action "b1"
action: 1

# State 32
# Apply action "a6"
action: 40

# State 33
# Apply action "g7"
action: 54

# State 34
# Apply action "a8"
action: 56

# State 35
# Apply action "a5"
action: 32

# State 36
# Apply action "g3"
action: 22

# State 37
# Apply action "b8"
action: 57

# State 38
# Apply action "g5"
action: 38

# State 39
# Apply action "d1"
action: 3

# State 40
# Black (x) to play:
#   a b c d e f g h
# 1 x o o o - - - - 1
# 2 x o - - - o - - 2
# 3 x o o - - o x - 3
# 4 - o x o o x - - 4
# 5 o o o o x x x - 5
# 6 x o x x x x - - 6
# 7 x o o o x o o - 7
# 8 x o x x x x x - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3]
HistoryString() = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3"
InformationStateString(1) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3"
ObservationString(0) = "Black (x) to play:\n  a b c d e f g h  \n1 x o o o - - - - 1\n2 x o - - - o - - 2\n3 x o o - - o x - 3\n4 - o x o o x - - 4\n5 o o o o x x x - 5\n6 x o x x x x - - 6\n7 x o o o x o o - 7\n8 x o x x x x x - 8\n  a b c d e f g h  "
ObservationString(1) = "Black (x) to play:\n  a b c d e f g h  \n1 x o o o - - - - 1\n2 x o - - - o - - 2\n3 x o o - - o x - 3\n4 - o x o o x - - 4\n5 o o o o x x x - 5\n6 x o x x x x - - 6\n7 x o o o x o o - 7\n8 x o x x x x x - 8\n  a b c d e f g h  "
PublicObservationString() = "Black (x) to play:\n  a b c d e f g h  \n1 x o o o - - - - 1\n2 x o - - - o - - 2\n3 x o o - - o x - 3\n4 - o x o o x - - 4\n5 o o o o x x x - 5\n6 x o x x x x - - 6\n7 x o o o x o o - 7\n8 x o x x x x x - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◯◯◯◯◉◉◉◉  ◉◯◯◯◯◯◯◯  ◯◉◉◉◯◯◯◯
◯◯◉◉◉◯◉◉  ◉◯◯◯◯◯◯◯  ◯◉◯◯◯◉◯◯
◯◯◯◉◉◯◯◉  ◉◯◯◯◯◯◉◯  ◯◉◉◯◯◉◯◯
◉◯◯◯◯◯◉◉  ◯◯◉◯◯◉◯◯  ◯◉◯◉◉◯◯◯
◯◯◯◯◯◯◯◉  ◯◯◯◯◉◉◉◯  ◉◉◉◉◯◯◯◯
◯◯◯◯◯◯◉◉  ◉◯◉◉◉◉◯◯  ◯◉◯◯◯◯◯◯
◯◯◯◯◯◯◯◉  ◉◯◯◯◉◯◯◯  ◯◉◉◉◯◉◉◯
◯◯◯◯◯◯◯◉  ◉◯◉◉◉◉◉◯  ◯◉◯◯◯◯◯◯
ObservationTensor(1):
◯◯◯◯◉◉◉◉  ◯◉◉◉◯◯◯◯  ◉◯◯◯◯◯◯◯
◯◯◉◉◉◯◉◉  ◯◉◯◯◯◉◯◯  ◉◯◯◯◯◯◯◯
◯◯◯◉◉◯◯◉  ◯◉◉◯◯◉◯◯  ◉◯◯◯◯◯◉◯
◉◯◯◯◯◯◉◉  ◯◉◯◉◉◯◯◯  ◯◯◉◯◯◉◯◯
◯◯◯◯◯◯◯◉  ◉◉◉◉◯◯◯◯  ◯◯◯◯◉◉◉◯
◯◯◯◯◯◯◉◉  ◯◉◯◯◯◯◯◯  ◉◯◉◉◉◉◯◯
◯◯◯◯◯◯◯◉  ◯◉◉◉◯◉◉◯  ◉◯◯◯◉◯◯◯
◯◯◯◯◯◯◯◉  ◯◉◯◯◯◯◯◯  ◉◯◉◉◉◉◉◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [4, 5, 10, 14, 19, 20, 24, 46, 47, 55, 63]
StringLegalActions() = ["e1", "f1", "c2", "g2", "d3", "e3", "a4", "g6", "h6", "h7", "h8"]

# Apply action "g2"
action: 14

# State 41
# White (o) to play:
#   a b c d e f g h
# 1 x o o o - - - - 1
# 2 x o - - - o x - 2
# 3 x o o - - x x - 3
# 4 - o x o x x - - 4
# 5 o o o x x x x - 5
# 6 x o x x x x - - 6
# 7 x o o o x o o - 7
# 8 x o x x x x x - 8
#   a b c d e f g h
IsTerminal() = False
History() = [44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14]
HistoryString() = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14"
InformationStateString(1) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14"
ObservationString(0) = "White (o) to play:\n  a b c d e f g h  \n1 x o o o - - - - 1\n2 x o - - - o x - 2\n3 x o o - - x x - 3\n4 - o x o x x - - 4\n5 o o o x x x x - 5\n6 x o x x x x - - 6\n7 x o o o x o o - 7\n8 x o x x x x x - 8\n  a b c d e f g h  "
ObservationString(1) = "White (o) to play:\n  a b c d e f g h  \n1 x o o o - - - - 1\n2 x o - - - o x - 2\n3 x o o - - x x - 3\n4 - o x o x x - - 4\n5 o o o x x x x - 5\n6 x o x x x x - - 6\n7 x o o o x o o - 7\n8 x o x x x x x - 8\n  a b c d e f g h  "
PublicObservationString() = "White (o) to play:\n  a b c d e f g h  \n1 x o o o - - - - 1\n2 x o - - - o x - 2\n3 x o o - - x x - 3\n4 - o x o x x - - 4\n5 o o o x x x x - 5\n6 x o x x x x - - 6\n7 x o o o x o o - 7\n8 x o x x x x x - 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◯◯◯◯◉◉◉◉  ◉◯◯◯◯◯◯◯  ◯◉◉◉◯◯◯◯
◯◯◉◉◉◯◯◉  ◉◯◯◯◯◯◉◯  ◯◉◯◯◯◉◯◯
◯◯◯◉◉◯◯◉  ◉◯◯◯◯◉◉◯  ◯◉◉◯◯◯◯◯
◉◯◯◯◯◯◉◉  ◯◯◉◯◉◉◯◯  ◯◉◯◉◯◯◯◯
◯◯◯◯◯◯◯◉  ◯◯◯◉◉◉◉◯  ◉◉◉◯◯◯◯◯
◯◯◯◯◯◯◉◉  ◉◯◉◉◉◉◯◯  ◯◉◯◯◯◯◯◯
◯◯◯◯◯◯◯◉  ◉◯◯◯◉◯◯◯  ◯◉◉◉◯◉◉◯
◯◯◯◯◯◯◯◉  ◉◯◉◉◉◉◉◯  ◯◉◯◯◯◯◯◯
ObservationTensor(1):
◯◯◯◯◉◉◉◉  ◯◉◉◉◯◯◯◯  ◉◯◯◯◯◯◯◯
◯◯◉◉◉◯◯◉  ◯◉◯◯◯◉◯◯  ◉◯◯◯◯◯◉◯
◯◯◯◉◉◯◯◉  ◯◉◉◯◯◯◯◯  ◉◯◯◯◯◉◉◯
◉◯◯◯◯◯◉◉  ◯◉◯◉◯◯◯◯  ◯◯◉◯◉◉◯◯
◯◯◯◯◯◯◯◉  ◉◉◉◯◯◯◯◯  ◯◯◯◉◉◉◉◯
◯◯◯◯◯◯◉◉  ◯◉◯◯◯◯◯◯  ◉◯◉◉◉◉◯◯
◯◯◯◯◯◯◯◉  ◯◉◉◉◯◉◉◯  ◉◯◯◯◉◯◯◯
◯◯◯◯◯◯◯◉  ◯◉◯◯◯◯◯◯  ◉◯◉◉◉◉◉◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, 0.0]
LegalActions() = [7, 15, 19, 30, 31, 39, 46, 63]
StringLegalActions() = ["h1", "h2", "d3", "g4", "h4", "h5", "g6", "h8"]

# Apply action "d3"
action: 19

# State 42
# Apply action "e2"
action: 12

# State 43
# Apply action "h8"
action: 63

# State 44
# Apply action "e3"
action: 20

# State 45
# Apply action "h4"
action: 31

# State 46
# Apply action "h5"
action: 39

# State 47
# Apply action "a4"
action: 24

# State 48
# Apply action "h3"
action: 23

# State 49
# Apply action "d2"
action: 11

# State 50
# Apply action "g6"
action: 46

# State 51
# Apply action "g4"
action: 30

# State 52
# Apply action "c2"
action: 10

# State 53
# Apply action "h2"
action: 15

# State 54
# Apply action "h6"
action: 47

# State 55
# Apply action "f1"
action: 5

# State 56
# Apply action "g1"
action: 6

# State 57
# Apply action "e1"
action: 4

# State 58
# Apply action "h1"
action: 7

# State 59
# Apply action "h7"
action: 55

# State 60
# Terminal State:
#   a b c d e f g h
# 1 x o o o o o x x 1
# 2 x x x o o o x x 2
# 3 x o o o x o o x 3
# 4 o o x o o x o x 4
# 5 o o x o x o x x 5
# 6 x x o o o x o x 6
# 7 x o o o o o o o 7
# 8 x o o o o o o o 8
#   a b c d e f g h
IsTerminal() = True
History() = [44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14, 19, 12, 63, 20, 31, 39, 24, 23, 11, 46, 30, 10, 15, 47, 5, 6, 4, 7, 55]
HistoryString() = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14, 19, 12, 63, 20, 31, 39, 24, 23, 11, 46, 30, 10, 15, 47, 5, 6, 4, 7, 55"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14, 19, 12, 63, 20, 31, 39, 24, 23, 11, 46, 30, 10, 15, 47, 5, 6, 4, 7, 55"
InformationStateString(1) = "44, 43, 26, 29, 45, 25, 17, 52, 21, 13, 37, 42, 41, 53, 34, 8, 16, 50, 0, 48, 61, 60, 59, 33, 18, 51, 58, 9, 2, 49, 62, 1, 40, 54, 56, 32, 22, 57, 38, 3, 14, 19, 12, 63, 20, 31, 39, 24, 23, 11, 46, 30, 10, 15, 47, 5, 6, 4, 7, 55"
ObservationString(0) = "Terminal State:\n  a b c d e f g h  \n1 x o o o o o x x 1\n2 x x x o o o x x 2\n3 x o o o x o o x 3\n4 o o x o o x o x 4\n5 o o x o x o x x 5\n6 x x o o o x o x 6\n7 x o o o o o o o 7\n8 x o o o o o o o 8\n  a b c d e f g h  "
ObservationString(1) = "Terminal State:\n  a b c d e f g h  \n1 x o o o o o x x 1\n2 x x x o o o x x 2\n3 x o o o x o o x 3\n4 o o x o o x o x 4\n5 o o x o x o x x 5\n6 x x o o o x o x 6\n7 x o o o o o o o 7\n8 x o o o o o o o 8\n  a b c d e f g h  "
PublicObservationString() = "Terminal State:\n  a b c d e f g h  \n1 x o o o o o x x 1\n2 x x x o o o x x 2\n3 x o o o x o o x 3\n4 o o x o o x o x 4\n5 o o x o x o x x 5\n6 x x o o o x o x 6\n7 x o o o o o o o 7\n8 x o o o o o o o 8\n  a b c d e f g h  "
PrivateObservationString(0) = ""
PrivateObservationString(1) = ""
ObservationTensor(0):
◯◯◯◯◯◯◯◯  ◉◯◯◯◯◯◉◉  ◯◉◉◉◉◉◯◯
◯◯◯◯◯◯◯◯  ◉◉◉◯◯◯◉◉  ◯◯◯◉◉◉◯◯
◯◯◯◯◯◯◯◯  ◉◯◯◯◉◯◯◉  ◯◉◉◉◯◉◉◯
◯◯◯◯◯◯◯◯  ◯◯◉◯◯◉◯◉  ◉◉◯◉◉◯◉◯
◯◯◯◯◯◯◯◯  ◯◯◉◯◉◯◉◉  ◉◉◯◉◯◉◯◯
◯◯◯◯◯◯◯◯  ◉◉◯◯◯◉◯◉  ◯◯◉◉◉◯◉◯
◯◯◯◯◯◯◯◯  ◉◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉◉
◯◯◯◯◯◯◯◯  ◉◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉◉
ObservationTensor(1):
◯◯◯◯◯◯◯◯  ◯◉◉◉◉◉◯◯  ◉◯◯◯◯◯◉◉
◯◯◯◯◯◯◯◯  ◯◯◯◉◉◉◯◯  ◉◉◉◯◯◯◉◉
◯◯◯◯◯◯◯◯  ◯◉◉◉◯◉◉◯  ◉◯◯◯◉◯◯◉
◯◯◯◯◯◯◯◯  ◉◉◯◉◉◯◉◯  ◯◯◉◯◯◉◯◉
◯◯◯◯◯◯◯◯  ◉◉◯◉◯◉◯◯  ◯◯◉◯◉◯◉◉
◯◯◯◯◯◯◯◯  ◯◯◉◉◉◯◉◯  ◉◉◯◯◯◉◯◉
◯◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉◉  ◉◯◯◯◯◯◯◯
◯◯◯◯◯◯◯◯  ◯◉◉◉◉◉◉◉  ◉◯◯◯◯◯◯◯
Rewards() = [-1.0, 1.0]
Returns() = [-1.0, 1.0]
