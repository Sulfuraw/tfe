game: dark_hex(row_size=5,col_size=4)

GameType.chance_mode = ChanceMode.DETERMINISTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.IMPERFECT_INFORMATION
GameType.long_name = "Dark Hex"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["board_size", "col_size", "gameversion", "obstype", "row_size"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = True
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "dark_hex"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 20
PolicyTensorShape() = [20]
MaxChanceOutcomes() = 0
GetParameters() = {board_size=11,col_size=4,gameversion=cdh,obstype=reveal-nothing,row_size=5}
NumPlayers() = 2
MinUtility() = -1.0
MaxUtility() = 1.0
UtilitySum() = 0.0
InformationStateTensorShape() = [1038]
InformationStateTensorLayout() = TensorLayout.CHW
InformationStateTensorSize() = 1038
ObservationTensorShape() = [180]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 180
MaxGameLength() = 39
ToString() = "dark_hex(col_size=4,row_size=5)"

# State 0
# . . . .
#  . . . .
#   . . . .
#    . . . .
#     . . . .
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = ".....\n.....\n.....\n.....\n0\n"
InformationStateString(1) = ".....\n.....\n.....\n.....\n0\n"
InformationStateTensor(0): binvec(1038, 0x2010080402010080402010080402010080402010080400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x2010080402010080402010080402010080402010080400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = ".....\n.....\n.....\n....."
ObservationString(1) = ".....\n.....\n.....\n....."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
ObservationTensor(1): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
StringLegalActions() = ["y(0,0)", "y(1,0)", "y(2,0)", "y(3,0)", "y(0,0)", "x(1,1)", "x(2,1)", "x(3,1)", "x(0,1)", "x(1,1)", "x(2,2)", "x(3,2)", "x(0,2)", "x(1,2)", "x(2,2)", "z(3,3)", "z(0,3)", "z(1,3)", "z(2,3)", "z(3,3)"]

# Apply action "y(2,0)"
action: 2

# State 1
# . . y .
#  . . . .
#   . . . .
#    . . . .
#     . . . .
IsTerminal() = False
History() = [2]
HistoryString() = "2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "..y..\n.....\n.....\n.....\n1\n0,2 "
InformationStateString(1) = ".....\n.....\n.....\n.....\n1\n"
InformationStateTensor(0): binvec(1038, 0x2010010402010080402010080402010080402010080404000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x2010080402010080402010080402010080402010080400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y..\n.....\n.....\n....."
ObservationString(1) = ".....\n.....\n.....\n....."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
ObservationTensor(1): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
StringLegalActions() = ["p(0,0)", "o(1,0)", "o(2,0)", "o(3,0)", "q(0,0)", "p(1,1)", "o(2,1)", "o(3,1)", "o(0,1)", "q(1,1)", "p(2,2)", "o(3,2)", "o(0,2)", "o(1,2)", "q(2,2)", "p(3,3)", "o(0,3)", "o(1,3)", "o(2,3)", "q(3,3)"]

# Apply action "p(2,2)"
action: 10

# State 2
# . . y .
#  . . . .
#   . . p .
#    . . . .
#     . . . .
IsTerminal() = False
History() = [2, 10]
HistoryString() = "2, 10"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "..y..\n.....\n.....\n.....\n2\n0,2 "
InformationStateString(1) = ".....\n.....\np....\n.....\n2\n1,10 "
InformationStateTensor(0): binvec(1038, 0x2010010402010080402010080402010080402010080404000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x2010080402010080402010400402010080402010080400000080100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y..\n.....\n.....\n....."
ObservationString(1) = ".....\n.....\np....\n....."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
ObservationTensor(1): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
StringLegalActions() = ["y(0,0)", "y(1,0)", "y(3,0)", "y(0,0)", "x(1,1)", "y(2,1)", "y(3,1)", "x(0,1)", "x(1,1)", "x(2,2)", "x(3,2)", "x(0,2)", "x(1,2)", "x(2,2)", "z(3,3)", "z(0,3)", "z(1,3)", "z(2,3)", "z(3,3)"]

# Apply action "z(3,3)"
action: 15

# State 3
# . . y .
#  . . . .
#   . . p .
#    . . . z
#     . . . .
IsTerminal() = False
History() = [2, 10, 15]
HistoryString() = "2, 10, 15"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "..y..\n.....\n.....\nz....\n3\n0,2 0,15 "
InformationStateString(1) = ".....\n.....\np....\n.....\n3\n1,10 "
InformationStateTensor(0): binvec(1038, 0x2010010402010080402010080402010080102010080404000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x2010080402010080402010400402010080402010080400000080100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y..\n.....\n.....\nz...."
ObservationString(1) = ".....\n.....\np....\n....."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
ObservationTensor(1): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19]
StringLegalActions() = ["p(0,0)", "o(1,0)", "o(2,0)", "o(3,0)", "q(0,0)", "p(1,1)", "p(2,1)", "o(3,1)", "o(0,1)", "q(1,1)", "p(3,2)", "o(0,2)", "o(1,2)", "q(2,2)", "p(3,3)", "o(0,3)", "o(1,3)", "o(2,3)", "q(3,3)"]

# Apply action "o(1,2)"
action: 13

# State 4
# . . y .
#  . . . .
#   . . p .
#    . o . z
#     . . . .
IsTerminal() = False
History() = [2, 10, 15, 13]
HistoryString() = "2, 10, 15, 13"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "..y..\n.....\n.....\nz....\n4\n0,2 0,15 "
InformationStateString(1) = ".....\n.....\np..o.\n.....\n4\n1,10 1,13 "
InformationStateTensor(0): binvec(1038, 0x2010010402010080402010080402010080102010080404000000000000020000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x2010080402010080402010400402020080402010080400000080100000000800200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y..\n.....\n.....\nz...."
ObservationString(1) = ".....\n.....\np..o.\n....."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
ObservationTensor(1): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19]
StringLegalActions() = ["y(0,0)", "y(1,0)", "y(3,0)", "y(0,0)", "x(1,1)", "y(2,1)", "y(3,1)", "x(0,1)", "x(1,1)", "z(2,2)", "z(3,2)", "x(0,2)", "x(1,2)", "x(2,2)", "z(0,3)", "z(1,3)", "z(2,3)", "z(3,3)"]

# Apply action "z(3,2)"
action: 11

# State 5
# . . y .
#  . . . .
#   . . p z
#    . o . z
#     . . . .
IsTerminal() = False
History() = [2, 10, 15, 13, 11]
HistoryString() = "2, 10, 15, 13, 11"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "..y..\n.....\n.z...\nz....\n5\n0,2 0,15 0,11 "
InformationStateString(1) = ".....\n.....\np..o.\n.....\n5\n1,10 1,13 "
InformationStateTensor(0): binvec(1038, 0x2010010402010080402010080102010080102010080404000000000000020000000002000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x2010080402010080402010400402020080402010080400000080100000000800200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y..\n.....\n.z...\nz...."
ObservationString(1) = ".....\n.....\np..o.\n....."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
ObservationTensor(1): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19]
StringLegalActions() = ["p(0,0)", "o(1,0)", "o(2,0)", "o(3,0)", "q(0,0)", "p(1,1)", "p(2,1)", "o(3,1)", "o(0,1)", "q(1,1)", "p(3,2)", "o(0,2)", "q(2,2)", "p(3,3)", "o(0,3)", "o(1,3)", "o(2,3)", "q(3,3)"]

# Apply action "o(0,1)"
action: 8

# State 6
# Apply action "z(3,3)"
action: 19

# State 7
# Apply action "p(3,3)"
action: 15

# State 8
# Apply action "p(3,2)"
action: 11

# State 9
# Apply action "p(2,1)"
action: 6

# State 10
# Apply action "x(1,1)"
action: 5

# State 11
# Apply action "p(0,0)"
action: 0

# State 12
# Apply action "z(2,3)"
action: 18

# State 13
# Apply action "p(1,1)"
action: 5

# State 14
# Apply action "o(3,0)"
action: 3

# State 15
# Apply action "z(0,2)"
action: 12

# State 16
# Apply action "q(2,2)"
action: 14

# State 17
# Apply action "y(0,0)"
action: 4

# State 18
# p . y q
#  y x p .
#   q . p z
#    z q q z
#     . . z z
IsTerminal() = False
History() = [2, 10, 15, 13, 11, 8, 19, 15, 11, 6, 5, 0, 18, 5, 3, 12, 14, 4]
HistoryString() = "2, 10, 15, 13, 11, 8, 19, 15, 11, 6, 5, 0, 18, 5, 3, 12, 14, 4"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "..y.y\nx....\n.zz..\nz..zz\n18\n0,2 0,15 0,11 0,19 0,5 0,18 0,12 0,4 "
InformationStateString(1) = "p..o.\nxp.o.\npz.oq\nz....\n18\n1,10 1,13 1,8 1,15 1,11 1,6 1,0 1,5 1,3 1,14 "
InformationStateTensor(0): binvec(1038, 0x2010010400408080402010080100810080102010020104000000000000020000000002000000000000200000000000000000080000000000000400000000000000400000000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x1001008080200840040401040010202020010201008040000008010000000080020000000804000000008000820020081000000000c000000000082000220000000002000400000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y.y\nx....\n.zz..\nz..zz"
ObservationString(1) = "p..o.\nxp.o.\npz.oq\nz...."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯
ObservationTensor(1): ◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [1, 2, 4, 7, 9, 12, 16, 17, 18, 19]
StringLegalActions() = ["p(1,0)", "O(2,0)", "q(0,0)", "O(3,1)", "q(1,1)", "q(0,2)", "o(0,3)", "q(1,3)", "q(2,3)", "q(3,3)"]

# Apply action "q(1,1)"
action: 9

# State 19
# Apply action "X(2,1)"
action: 6

# State 20
# Apply action "X(0,1)"
action: 8

# State 21
# p . y q
#  y x p .
#   q q p z
#    z q q z
#     . . z z
IsTerminal() = False
History() = [2, 10, 15, 13, 11, 8, 19, 15, 11, 6, 5, 0, 18, 5, 3, 12, 14, 4, 9, 6, 8]
HistoryString() = "2, 10, 15, 13, 11, 8, 19, 15, 11, 6, 5, 0, 18, 5, 3, 12, 14, 4, 9, 6, 8"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "..y.y\nxp.q.\n.zz..\nz..zz\n21\n0,2 0,15 0,11 0,19 0,5 0,18 0,12 0,4 0,6 0,8 "
InformationStateString(1) = "p..o.\nxp.oq\npz.oq\nz....\n21\n1,10 1,13 1,8 1,15 1,11 1,6 1,0 1,5 1,3 1,14 1,9 "
InformationStateTensor(0): binvec(1038, 0x2010010400408400408010080100810080102010020104000000000000020000000002000000000000200000000000000000080000000000000400000000000000400000000400000000001000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x1001008080200840040404040010202020010201008040000008010000000080020000000804000000008000820020081000000000c000000000082000220000000002000400000020080000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000)
ObservationString(0) = "..y.y\nxp.q.\n.zz..\nz..zz"
ObservationString(1) = "p..o.\nxp.oq\npz.oq\nz...."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯
ObservationTensor(1): ◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [0.0, 0.0]
Returns() = [0.0, -0.0]
LegalActions() = [0, 1, 3, 7, 9, 10, 13, 14, 16, 17]
StringLegalActions() = ["y(0,0)", "y(1,0)", "y(3,0)", "X(3,1)", "y(1,1)", "z(2,2)", "z(1,2)", "z(2,2)", "z(0,3)", "z(1,3)"]

# Apply action "y(1,1)"
action: 9

# State 22
# Apply action "y(1,0)"
action: 1

# State 23
# Apply action "q(1,3)"
action: 17

# State 24
# Apply action "z(1,3)"
action: 17

# State 25
# Apply action "z(2,2)"
action: 14

# State 26
# Apply action "y(3,0)"
action: 3

# State 27
# Apply action "X(2,2)"
action: 10

# State 28
# Apply action "z(1,2)"
action: 13

# State 29
# Apply action "z(0,3)"
action: 16

# State 30
# Apply action "O(3,1)"
action: 7

# State 31
# p y y q
#  y y p O
#   q q p z
#    z q q z
#     z q z z
IsTerminal() = True
History() = [2, 10, 15, 13, 11, 8, 19, 15, 11, 6, 5, 0, 18, 5, 3, 12, 14, 4, 9, 6, 8, 9, 1, 17, 17, 14, 3, 10, 13, 16, 7]
HistoryString() = "2, 10, 15, 13, 11, 8, 19, 15, 11, 6, 5, 0, 18, 5, 3, 12, 14, 4, 9, 6, 8, 9, 1, 17, 17, 14, 3, 10, 13, 16, 7"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = -4
InformationStateString(0) = ".yyqy\nxp.qq\npzzqq\nzzqzz\n31\n0,2 0,15 0,11 0,19 0,5 0,18 0,12 0,4 0,6 0,8 0,9 0,1 0,17 0,14 0,3 0,10 0,13 0,16 "
InformationStateString(1) = "p..o.\nxpOoq\npz.oq\nz.q..\n31\n1,10 1,13 1,8 1,15 1,11 1,6 1,0 1,5 1,3 1,14 1,9 1,17 1,7 "
InformationStateTensor(0): binvec(1038, 0x2002011000408400408040400100840200100840020104000000000000020000000002000000000000200000000000000000080000000000000400000000000000400000000400000000001000001000002000800000000000000800010020000001000000800000400000000000000000000000000000000000000000000000000)
InformationStateTensor(1): binvec(1038, 0x1001008080200840400404040010202020010204008040000008010000000080020000000804000000008000820020081000000000c000000000082000220000000002000400000020080000000000000000000000008000200000000000000000000000000000000020200000000000000000000000000000000000000000000000)
ObservationString(0) = ".yyqy\nxp.qq\npzzqq\nzzqzz"
ObservationString(1) = "p..o.\nxpOoq\npz.oq\nz.q.."
ObservationTensor(0): ◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯
ObservationTensor(1): ◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◉◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◯◯◉◯◯◯◯◯◯◯◯◉◯◯◯◯
Rewards() = [-1.0, 1.0]
Returns() = [-1.0, 1.0]
